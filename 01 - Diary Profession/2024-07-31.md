Data Ninja: (Challenges)
- LAUSD: Pods crashing and causing sessions to go away and random users are impacted. This is a blocker to auto scaling. Else costs will go high. Tomcat Cluster. Whenever a pod is removed from a cluster then it recalculates hash for users. This causes random users to be kick out of the system. There is a method stateful session with strong affinity that team is trying to use. With ALB its easy. But with ISTIO there are challenges. ISTIO with EKS its not supported well. Documentation is provided but team is trying to figure this out. No other team as such is using it. 
- NewRelic Alerts for POD Resource Usage (CPU/Memory) and Restarts or Crashes. 
- Montana: Per district EFS is not available. Number of Access points needed is high. This is a code based change in Argo Workflow. Upgrade workflow will mount EFS in each pod for each district. 
- Montana: ETCDB getting full. Outage because of Argo workflow size. Argo workflow template is exported as an environment variable in each pod. Its making each pod size very big. Call with AWS provided us this information very clearly. To solve this , we need to reduce the pod spec. POD specs to be stored in Minio (its kind of s3 bucket technology) part of K8S and then call it. This is already been used.
- Keep hard stand on CloudOps
- GitLab Runner Changes: Moving to Ubuntu. There are failures in many places which need to be fixed. 
Snowstorm: 
- Guided team for resolving several blockers from other teams. e.g. services work let team dig deeper and they did it for scsmustang and figured this out themselves. 
- Connected team with Farheen and Yasmeen for Assessment Tickets. 
- Guided team for logging separate ticket for each issue post migration they find and have story points. So that their story points in each sprint is consistent. 
- Team should give shoutout more esp the historical data copy is a good achievements. 
